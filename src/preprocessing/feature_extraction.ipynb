{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = \"/Users/feiyixie/Projects/Summer-2024-ECE-597-Group8\"\n",
    "normal_sample_numbers = 2571 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_extract_subject_body(text):\n",
    "    # Extract the subject using a regular expression\n",
    "    subject_match = re.search(r\"Subject: (.*)\", text)\n",
    "    subject = subject_match.group(1) if subject_match else \"Subject Not Found\"\n",
    "\n",
    "    # Clean the text by removing everything from \"Message-ID:\" up to \"X-FileName:\"\n",
    "    cleaned_text = re.sub(r\"Message-ID:.*?X-FileName:.*?\\n\", \"\", text, flags=re.S)\n",
    "\n",
    "    return subject, cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_data_path = os.path.join(\n",
    "    project_root, \"data\", \"raw\", \"CaptstoneProjectData_2024.csv\"\n",
    ")\n",
    "normal_data_path = os.path.join(\n",
    "    project_root, \"data\", \"raw\", \"EnronEmailDataset.csv\"\n",
    ")\n",
    "\n",
    "phishing_data = pd.read_csv(phishing_data_path)\n",
    "normal_data = pd.read_csv(\n",
    "    normal_data_path, nrows=normal_sample_numbers\n",
    ")\n",
    "\n",
    "phishing_data.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\"], inplace=True)\n",
    "phishing_data[\"Subject\"] = phishing_data[\"Subject\"].fillna(\"Subject Not Found\")\n",
    "phishing_data.dropna(\n",
    "    subset=[\"Body\"], inplace=True\n",
    ")\n",
    "phishing_data.reset_index(drop=True, inplace=True)\n",
    "normal_data[[\"Subject\", \"Body\"]] = normal_data[\"message\"].apply(\n",
    "    lambda x: pd.Series(normal_extract_subject_body(x))\n",
    ")\n",
    "\n",
    "phishing_data[\"Label\"] = 1\n",
    "normal_data[\"Label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28281 entries, 0 to 28280\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Subject  28281 non-null  object\n",
      " 1   Body     28281 non-null  object\n",
      " 2   Label    28281 non-null  int64 \n",
      " 3   file     25710 non-null  object\n",
      " 4   message  25710 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.1+ MB\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 28281 entries, 0 to 28280\n",
      "Series name: Label\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "28281 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 221.1 KB\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.concat([phishing_data, normal_data], ignore_index=True)\n",
    "df_data.info()\n",
    "df_labels = df_data[\"Label\"]\n",
    "df_labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/feiyixie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/feiyixie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/feiyixie/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/feiyixie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Subject  \\\n",
      "0      ®Review your shipment details / Shipment Notif...   \n",
      "1                                Υоur ассоunt іѕ оn hоld   \n",
      "2      Completed: Invoice # KZ89TYS2564  +1.813.776.1410   \n",
      "3                                 UVic IMPORTANT NOTICE!   \n",
      "4              You have (6) Suspended incoming messages    \n",
      "...                                                  ...   \n",
      "28276    Out of the Office - Afternoon of Friday, June 9   \n",
      "28277                                    Re: TDS project   \n",
      "28278                              Global Systems Matrix   \n",
      "28279             Mid Year Energy Operations PRC Meeting   \n",
      "28280                             Mgmt Summary & HotList   \n",
      "\n",
      "                                                    Body  Label  \\\n",
      "0       [] Dear , An agent of ours tried to ship a pa...      1   \n",
      "1       Votre réponse a bien été prise en compte. [] ...      1   \n",
      "2               [DocuSign] [] Your document has been ...      1   \n",
      "3      Your UVIC account has been filed under the lis...      1   \n",
      "4       Message generated from   source. Sender      ...      1   \n",
      "...                                                  ...    ...   \n",
      "28276   I will be out of the office after noon on Fri...      0   \n",
      "28277   You might hear of this, so I thought I better...      0   \n",
      "28278   Rick - You had asked a while back for a matri...      0   \n",
      "28279   Our mid year PRC meeting will be held Monday,...      0   \n",
      "28280   Attached are the Mgmt Summary, Hot List and M...      0   \n",
      "\n",
      "                                  file  \\\n",
      "0                                  NaN   \n",
      "1                                  NaN   \n",
      "2                                  NaN   \n",
      "3                                  NaN   \n",
      "4                                  NaN   \n",
      "...                                ...   \n",
      "28276  beck-s/discussion_threads/1158.   \n",
      "28277  beck-s/discussion_threads/1159.   \n",
      "28278  beck-s/discussion_threads/1160.   \n",
      "28279  beck-s/discussion_threads/1161.   \n",
      "28280  beck-s/discussion_threads/1162.   \n",
      "\n",
      "                                                 message  \\\n",
      "0                                                    NaN   \n",
      "1                                                    NaN   \n",
      "2                                                    NaN   \n",
      "3                                                    NaN   \n",
      "4                                                    NaN   \n",
      "...                                                  ...   \n",
      "28276  Message-ID: <8477157.1075855835539.JavaMail.ev...   \n",
      "28277  Message-ID: <3475775.1075855835565.JavaMail.ev...   \n",
      "28278  Message-ID: <32640873.1075855835587.JavaMail.e...   \n",
      "28279  Message-ID: <13745048.1075855835616.JavaMail.e...   \n",
      "28280  Message-ID: <24326924.1075855835645.JavaMail.e...   \n",
      "\n",
      "                                               Full_Text  Word_Count  \n",
      "0      ®Review your shipment details / Shipment Notif...         119  \n",
      "1      Υоur ассоunt іѕ оn hоld Votre réponse a bien é...         119  \n",
      "2      Completed: Invoice # KZ89TYS2564  +1.813.776.1...         261  \n",
      "3      UVic IMPORTANT NOTICE!Your UVIC account has be...          83  \n",
      "4      You have (6) Suspended incoming messages  Mess...         169  \n",
      "...                                                  ...         ...  \n",
      "28276  Out of the Office - Afternoon of Friday, June ...          62  \n",
      "28277  Re: TDS project You might hear of this, so I t...        1728  \n",
      "28278  Global Systems Matrix Rick - You had asked a w...          61  \n",
      "28279  Mid Year Energy Operations PRC Meeting Our mid...         206  \n",
      "28280  Mgmt Summary & HotList Attached are the Mgmt S...          18  \n",
      "\n",
      "[28281 rows x 7 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28281 entries, 0 to 28280\n",
      "Data columns (total 1 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   Word_Count  28281 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 221.1 KB\n"
     ]
    }
   ],
   "source": [
    "from get_email_length import fetch_email_length\n",
    "\n",
    "df_wordCount = fetch_email_length(df_data.copy())\n",
    "df_wordCount.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28281 entries, 0 to 28280\n",
      "Data columns (total 1 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   Homoglyphs  28281 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 221.1 KB\n"
     ]
    }
   ],
   "source": [
    "from homoglyphs import feature_homoglyphs\n",
    "\n",
    "df_homoglyphs = feature_homoglyphs(df_data.copy())\n",
    "df_homoglyphs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28281 entries, 0 to 28280\n",
      "Data columns (total 1 columns):\n",
      " #   Column                Non-Null Count  Dtype\n",
      "---  ------                --------------  -----\n",
      " 0   Total_Abnormal_Count  28281 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 221.1 KB\n"
     ]
    }
   ],
   "source": [
    "from abnormal_number_extract import extract_abnormal_number\n",
    "\n",
    "df_abnormal = extract_abnormal_number(df_data.copy())\n",
    "df_abnormal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28281 entries, 0 to 28280\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   html_tags  28281 non-null  int64\n",
      " 1   js_code    28281 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 442.0 KB\n"
     ]
    }
   ],
   "source": [
    "from html_JS import extract_features_from_message\n",
    "\n",
    "df_specials = df_data.copy().apply(\n",
    "    lambda row: extract_features_from_message(\n",
    "        str(row['Subject']) + ' ' + str(row['Body'])\n",
    "    ), axis=1\n",
    ")\n",
    "df_specials = pd.DataFrame(df_specials.tolist(), columns=['html_tags', 'js_code'])\n",
    "df_specials.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phishing_bow import feature_extract_bow\n",
    "\n",
    "df_bow = feature_extract_bow(df_data.copy())\n",
    "df_bow.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phishing_tfidf import feature_extract_tfidf\n",
    "\n",
    "df_tfidf = feature_extract_tfidf(df_data.copy())\n",
    "df_tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/feiyixie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/feiyixie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/feiyixie/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/feiyixie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28281 entries, 0 to 28280\n",
      "Data columns (total 100 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   word2Vec_0   28281 non-null  float64\n",
      " 1   word2Vec_1   28281 non-null  float64\n",
      " 2   word2Vec_2   28281 non-null  float64\n",
      " 3   word2Vec_3   28281 non-null  float64\n",
      " 4   word2Vec_4   28281 non-null  float64\n",
      " 5   word2Vec_5   28281 non-null  float64\n",
      " 6   word2Vec_6   28281 non-null  float64\n",
      " 7   word2Vec_7   28281 non-null  float64\n",
      " 8   word2Vec_8   28281 non-null  float64\n",
      " 9   word2Vec_9   28281 non-null  float64\n",
      " 10  word2Vec_10  28281 non-null  float64\n",
      " 11  word2Vec_11  28281 non-null  float64\n",
      " 12  word2Vec_12  28281 non-null  float64\n",
      " 13  word2Vec_13  28281 non-null  float64\n",
      " 14  word2Vec_14  28281 non-null  float64\n",
      " 15  word2Vec_15  28281 non-null  float64\n",
      " 16  word2Vec_16  28281 non-null  float64\n",
      " 17  word2Vec_17  28281 non-null  float64\n",
      " 18  word2Vec_18  28281 non-null  float64\n",
      " 19  word2Vec_19  28281 non-null  float64\n",
      " 20  word2Vec_20  28281 non-null  float64\n",
      " 21  word2Vec_21  28281 non-null  float64\n",
      " 22  word2Vec_22  28281 non-null  float64\n",
      " 23  word2Vec_23  28281 non-null  float64\n",
      " 24  word2Vec_24  28281 non-null  float64\n",
      " 25  word2Vec_25  28281 non-null  float64\n",
      " 26  word2Vec_26  28281 non-null  float64\n",
      " 27  word2Vec_27  28281 non-null  float64\n",
      " 28  word2Vec_28  28281 non-null  float64\n",
      " 29  word2Vec_29  28281 non-null  float64\n",
      " 30  word2Vec_30  28281 non-null  float64\n",
      " 31  word2Vec_31  28281 non-null  float64\n",
      " 32  word2Vec_32  28281 non-null  float64\n",
      " 33  word2Vec_33  28281 non-null  float64\n",
      " 34  word2Vec_34  28281 non-null  float64\n",
      " 35  word2Vec_35  28281 non-null  float64\n",
      " 36  word2Vec_36  28281 non-null  float64\n",
      " 37  word2Vec_37  28281 non-null  float64\n",
      " 38  word2Vec_38  28281 non-null  float64\n",
      " 39  word2Vec_39  28281 non-null  float64\n",
      " 40  word2Vec_40  28281 non-null  float64\n",
      " 41  word2Vec_41  28281 non-null  float64\n",
      " 42  word2Vec_42  28281 non-null  float64\n",
      " 43  word2Vec_43  28281 non-null  float64\n",
      " 44  word2Vec_44  28281 non-null  float64\n",
      " 45  word2Vec_45  28281 non-null  float64\n",
      " 46  word2Vec_46  28281 non-null  float64\n",
      " 47  word2Vec_47  28281 non-null  float64\n",
      " 48  word2Vec_48  28281 non-null  float64\n",
      " 49  word2Vec_49  28281 non-null  float64\n",
      " 50  word2Vec_50  28281 non-null  float64\n",
      " 51  word2Vec_51  28281 non-null  float64\n",
      " 52  word2Vec_52  28281 non-null  float64\n",
      " 53  word2Vec_53  28281 non-null  float64\n",
      " 54  word2Vec_54  28281 non-null  float64\n",
      " 55  word2Vec_55  28281 non-null  float64\n",
      " 56  word2Vec_56  28281 non-null  float64\n",
      " 57  word2Vec_57  28281 non-null  float64\n",
      " 58  word2Vec_58  28281 non-null  float64\n",
      " 59  word2Vec_59  28281 non-null  float64\n",
      " 60  word2Vec_60  28281 non-null  float64\n",
      " 61  word2Vec_61  28281 non-null  float64\n",
      " 62  word2Vec_62  28281 non-null  float64\n",
      " 63  word2Vec_63  28281 non-null  float64\n",
      " 64  word2Vec_64  28281 non-null  float64\n",
      " 65  word2Vec_65  28281 non-null  float64\n",
      " 66  word2Vec_66  28281 non-null  float64\n",
      " 67  word2Vec_67  28281 non-null  float64\n",
      " 68  word2Vec_68  28281 non-null  float64\n",
      " 69  word2Vec_69  28281 non-null  float64\n",
      " 70  word2Vec_70  28281 non-null  float64\n",
      " 71  word2Vec_71  28281 non-null  float64\n",
      " 72  word2Vec_72  28281 non-null  float64\n",
      " 73  word2Vec_73  28281 non-null  float64\n",
      " 74  word2Vec_74  28281 non-null  float64\n",
      " 75  word2Vec_75  28281 non-null  float64\n",
      " 76  word2Vec_76  28281 non-null  float64\n",
      " 77  word2Vec_77  28281 non-null  float64\n",
      " 78  word2Vec_78  28281 non-null  float64\n",
      " 79  word2Vec_79  28281 non-null  float64\n",
      " 80  word2Vec_80  28281 non-null  float64\n",
      " 81  word2Vec_81  28281 non-null  float64\n",
      " 82  word2Vec_82  28281 non-null  float64\n",
      " 83  word2Vec_83  28281 non-null  float64\n",
      " 84  word2Vec_84  28281 non-null  float64\n",
      " 85  word2Vec_85  28281 non-null  float64\n",
      " 86  word2Vec_86  28281 non-null  float64\n",
      " 87  word2Vec_87  28281 non-null  float64\n",
      " 88  word2Vec_88  28281 non-null  float64\n",
      " 89  word2Vec_89  28281 non-null  float64\n",
      " 90  word2Vec_90  28281 non-null  float64\n",
      " 91  word2Vec_91  28281 non-null  float64\n",
      " 92  word2Vec_92  28281 non-null  float64\n",
      " 93  word2Vec_93  28281 non-null  float64\n",
      " 94  word2Vec_94  28281 non-null  float64\n",
      " 95  word2Vec_95  28281 non-null  float64\n",
      " 96  word2Vec_96  28281 non-null  float64\n",
      " 97  word2Vec_97  28281 non-null  float64\n",
      " 98  word2Vec_98  28281 non-null  float64\n",
      " 99  word2Vec_99  28281 non-null  float64\n",
      "dtypes: float64(100)\n",
      "memory usage: 21.6 MB\n"
     ]
    }
   ],
   "source": [
    "from phishing_Word2Vec import feature_extract_word2Vec\n",
    "\n",
    "df_word2Vec = feature_extract_word2Vec(df_data.copy())\n",
    "df_word2Vec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.concat([df_wordCount, df_homoglyphs, df_abnormal, df_specials], axis=1)\n",
    "\n",
    "df_features_labels = pd.concat([df_features, df_labels], axis=1)\n",
    "df_bow_labels = pd.concat([df_bow, df_labels], axis=1)\n",
    "df_tfidf_labels = pd.concat([df_tfidf, df_labels], axis=1)\n",
    "df_features_bow_labels = pd.concat([df_features, df_bow, df_labels], axis=1)\n",
    "df_features_tfidf_labels = pd.concat([df_features, df_tfidf, df_labels], axis=1)\n",
    "df_features_word2Vec_labels = pd.concat([df_features, df_word2Vec, df_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_labels = df_features_labels.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_bow_labels = df_bow_labels.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_tfidf_labels = df_tfidf_labels.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_features_bow_labels = df_features_bow_labels.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_features_tfidf_labels = df_features_tfidf_labels.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_features_word2Vec_labels = df_features_word2Vec_labels.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "df_features_labels.to_csv(os.path.join(project_root, \"data\", \"processed\", \"features_labels.csv\"), index=False)\n",
    "df_bow_labels.to_csv(os.path.join(project_root, \"data\", \"processed\", \"bow_labels.csv\"), index=False)\n",
    "df_tfidf_labels.to_csv(os.path.join(project_root, \"data\", \"processed\", \"tfidf_labels.csv\"), index=False)\n",
    "df_features_bow_labels.to_csv(os.path.join(project_root, \"data\", \"processed\", \"features_bow_labels.csv\"), index=False)\n",
    "df_features_tfidf_labels.to_csv(os.path.join(project_root, \"data\", \"processed\", \"features_tfidf_labels.csv\"), index=False)\n",
    "df_features_word2Vec_labels.to_csv(os.path.join(project_root, \"data\", \"processed\", \"features_word2Vec_labels.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
